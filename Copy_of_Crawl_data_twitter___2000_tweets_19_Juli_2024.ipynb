{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEWpayxURuUn"
      },
      "source": [
        "# Crawl Data Twitter > 2000 Tweets\n",
        "The crawling process was done using Tweet-Harvest. Written by Helmi Satria on  March 30th 2024.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S00x_f6-GeD"
      },
      "outputs": [],
      "source": [
        "#@title Twitter Auth Token\n",
        "\n",
        "twitter_auth_token = 'd9e784a01b3be780f3b5b757e54b9b59173d91ad'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4UIL1x21P9rQ"
      },
      "outputs": [],
      "source": [
        "# Import required Python package\n",
        "!pip install pandas\n",
        "\n",
        "# Install Node.js (because tweet-harvest built using Node.js)\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ca-certificates curl gnupg\n",
        "!sudo mkdir -p /etc/apt/keyrings\n",
        "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
        "\n",
        "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install nodejs -y\n",
        "\n",
        "!node -v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-1EBrlediWE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYDR51dJlVlX",
        "outputId": "9c7a29cd-d846-4aa6-a87f-3a2e3a3a0771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\u001b[1m\u001b[32mTweet Harvest [v2.6.1]\u001b[39m\u001b[22m\n",
            "\u001b[1m\u001b[32m\u001b[39m\u001b[22m\n",
            "\u001b[34mResearch by \u001b[39m\u001b[1m\u001b[34mHelmi Satria\u001b[39m\u001b[22m\u001b[34m\u001b[39m\n",
            "\u001b[34mUse it for Educational Purposes only!\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[33mThis script uses Chromium Browser to crawl data from Twitter with \u001b[1myour Twitter auth token\u001b[22m.\u001b[39m\n",
            "\u001b[33mPlease enter your Twitter auth token when prompted.\u001b[39m\n",
            "\u001b[33m\u001b[39m\n",
            "\u001b[31m\u001b[1mNote:\u001b[22m\u001b[39m Keep your access token secret! Don't share it with anyone else.\n",
            "\u001b[31m\u001b[1mNote:\u001b[22m\u001b[39m This script only runs on your local device.\n",
            "\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34mOpening twitter search page...\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34mFound existing file ./tweets-data/pionir.csv, renaming to ./tweets-data/pionir.old.csv\u001b[39m\n",
            "\u001b[33m\u001b[39m\n",
            "\u001b[33mFilling in keywords: pionir since:2025-01-01 until:2025-06-01 lang:id\u001b[39m\n",
            "\u001b[33m\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[90m[v2.6.1]\u001b[39m Error parsing response json: {\"_type\":\"Response\",\"_guid\":\"response@fa0deaedd8b45ba36d97fec2d27aa9ec\"}\n",
            "\u001b[90m[v2.6.1]\u001b[39m Most likely, you have already exceeded the Twitter rate limit. Read more on https://x.com/elonmusk/status/1675187969420828672.\n",
            "\u001b[90m[v2.6.1]\u001b[39m Error parsing response json: {\"_type\":\"Response\",\"_guid\":\"response@84f8980947d3de02bc16057579a68e8e\"}\n",
            "\u001b[90m[v2.6.1]\u001b[39m Most likely, you have already exceeded the Twitter rate limit. Read more on https://x.com/elonmusk/status/1675187969420828672.\n",
            "\u001b[90m[v2.6.1]\u001b[39m Error parsing response json: {\"_type\":\"Response\",\"_guid\":\"response@a1bc2071b38041ebf9302b31bf79c26a\"}\n",
            "\u001b[90m[v2.6.1]\u001b[39m Most likely, you have already exceeded the Twitter rate limit. Read more on https://x.com/elonmusk/status/1675187969420828672.\n",
            "\u001b[90m[v2.6.1]\u001b[39m Error parsing response json: {\"_type\":\"Response\",\"_guid\":\"response@1a93a6b4c00ba074a761e748e441d3f5\"}\n",
            "\u001b[90m[v2.6.1]\u001b[39m Most likely, you have already exceeded the Twitter rate limit. Read more on https://x.com/elonmusk/status/1675187969420828672.\n",
            "\u001b[90m[v2.6.1]\u001b[39m Error parsing response json: {\"_type\":\"Response\",\"_guid\":\"response@3bde369c32d12486f3e99ef95935bfa5\"}\n",
            "\u001b[90m[v2.6.1]\u001b[39m Most likely, you have already exceeded the Twitter rate limit. Read more on https://x.com/elonmusk/status/1675187969420828672.\n",
            "\u001b[90m[v2.6.1]\u001b[39m Error parsing response json: {\"_type\":\"Response\",\"_guid\":\"response@b6c20b447f95017a72fc29184ac9fbec\"}\n",
            "\u001b[90m[v2.6.1]\u001b[39m Most likely, you have already exceeded the Twitter rate limit. Read more on https://x.com/elonmusk/status/1675187969420828672.\n"
          ]
        }
      ],
      "source": [
        "# Crawl Data\n",
        "\n",
        "filename = 'pionir.csv'\n",
        "search_keyword = 'pionir since:2025-01-01 until:2025-06-01 lang:id'\n",
        "limit = 100\n",
        "\n",
        "!npx -y tweet-harvest@2.6.1 -o \"{filename}\" -s \"{search_keyword}\" --tab \"LATEST\" -l {limit} --token {twitter_auth_token}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvAG3hPvQDqk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "file_path = f\"tweets-data/{filename}\"\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(file_path, delimiter=\",\")\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df['full_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRfDl54waHC4"
      },
      "outputs": [],
      "source": [
        "# Cek jumlah data yang didapatkan\n",
        "\n",
        "num_tweets = len(df)\n",
        "print(f\"Jumlah tweet dalam dataframe adalah {num_tweets}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jevnZWJjDJmi"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned')\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned')\n",
        "\n",
        "sentiment_task = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "texts = list(df.content.values)\n",
        "\n",
        "results = sentiment_task(texts)\n",
        "\n",
        "df['sentiment'] = [r[0]['label'] for r in results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYtviXjRFPuN"
      },
      "outputs": [],
      "source": [
        "for text, results, score in zip(texts, results, df.sentiment):\n",
        "  print(f\"Text: {text}\\nSentiment: {results}\\nScore: {score}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
